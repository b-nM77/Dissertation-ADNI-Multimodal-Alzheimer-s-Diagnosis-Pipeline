{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b0cab5",
   "metadata": {},
   "source": [
    "\n",
    "# ADNI Diagnosis — Full Fusion Pipeline (ANOVA + ADASYN)\n",
    "\n",
    "This notebook trains multi-class diagnostic models (CN / MCI / DEMENTIA) on your **full fusion** dataset.\n",
    "\n",
    "**What it does**\n",
    "- Loads `fusion_master.xlsx` and detects `PTID` and `Diagnosis`.\n",
    "- Drops rows with missing `Diagnosis` and removes any pre-existing one-hot `diagnosis_*` columns from features to prevent leakage.\n",
    "- Encodes Diagnosis → integers (CN=0, MCI=1, DEMENTIA=2).\n",
    "- Preprocesses: median-impute numerics + standardize, one-hot encode categoricals.\n",
    "- **Feature selection**: ANOVA (SelectKBest(f_classif)).\n",
    "- **Class imbalance**: ADASYN inside CV folds (no leakage).\n",
    "- Trains models: **RandomForest, XGBoost, SVM**, and a **Stacking Ensemble**.\n",
    "- Evaluates with **Accuracy, Macro-F1, Macro ROC-AUC**, and **confusion matrices**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5939b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === User config ===\n",
    "from pathlib import Path\n",
    "FUSION_XLSX = r\"/Users/madhurabn/Desktop/adni/data/processed/fusion_master.xlsx\"  # <--- set to your path\n",
    "OUTDIR = Path(\"/Users/madhurabn/Desktop/adni/out/full_fusion\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "K_SELECTED = 30  # number of features to keep via ANOVA\n",
    "\n",
    "print(\"Output dir:\", OUTDIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dfdb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# plotting inline & style\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn / imblearn\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# xgboost (must be installed in your environment)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception as e:\n",
    "    print(\"xgboost not available:\", e)\n",
    "    HAS_XGB = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e63e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Load fusion master ===\n",
    "df = pd.read_excel(FUSION_XLSX)\n",
    "print(\"Loaded:\", df.shape)\n",
    "print(\"Columns (head):\", df.columns[:15].tolist())\n",
    "\n",
    "# --- find PTID and Diagnosis columns ---\n",
    "def _first_match(cols, tokens):\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    for c in cols:\n",
    "        lc = str(c).lower()\n",
    "        if any(t in lc for t in tokens):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "ptid_col = _first_match(df.columns, [\"ptid\",\"rid\",\"subject\"])\n",
    "diag_col = _first_match(df.columns, [\"diagnosis\",\"dx\",\"dxgroup\",\"clinical_diagnosis\"])\n",
    "\n",
    "if ptid_col is None or diag_col is None:\n",
    "    raise ValueError(f\"Could not find PTID or Diagnosis. Found PTID={ptid_col}, Diagnosis={diag_col}\")\n",
    "\n",
    "print(\"Detected PTID column:\", ptid_col)\n",
    "print(\"Detected Diagnosis column:\", diag_col)\n",
    "\n",
    "# drop rows with missing diagnosis\n",
    "before = df.shape[0]\n",
    "df = df[~df[diag_col].isna()].copy()\n",
    "print(f\"Dropped {before - df.shape[0]} rows with empty {diag_col}. Current shape: {df.shape}\")\n",
    "\n",
    "# remove any pre-existing one-hot diagnosis_* columns from features\n",
    "import re\n",
    "drop_diag_dummies = [c for c in df.columns if re.match(r\"(?i)diagnosis[_\\- ]?(cn|mci|ad|dementia)\", str(c))]\n",
    "if drop_diag_dummies:\n",
    "    print(\"Dropping pre-existing diagnosis dummy columns from features:\", drop_diag_dummies)\n",
    "    df = df.drop(columns=drop_diag_dummies)\n",
    "\n",
    "# map diagnosis to integers {CN:0, MCI:1, DEMENTIA:2} (case-insensitive)\n",
    "def _map_diag(v):\n",
    "    if pd.isna(v): return np.nan\n",
    "    s = str(v).strip().lower()\n",
    "    if s in {\"cn\", \"control\", \"normal\"}: return 0\n",
    "    if s in {\"mci\"}: return 1\n",
    "    if s in {\"ad\", \"dementia\", \"alzheimers\", \"alzheimer's\"}: return 2\n",
    "    return np.nan\n",
    "\n",
    "y_int = df[diag_col].map(_map_diag)\n",
    "if y_int.isna().any():\n",
    "    bad = df.loc[y_int.isna(), diag_col].unique()[:10]\n",
    "    raise ValueError(f\"Unknown diagnosis labels encountered: {bad}. Please adjust mapping.\")\n",
    "\n",
    "y = y_int.values.astype(int)\n",
    "\n",
    "# X = all non-target columns except obvious IDs / meta\n",
    "meta_like = {ptid_col, diag_col, \"visit\", \"viscode\"}\n",
    "X = df.drop(columns=[c for c in df.columns if c in meta_like]).copy()\n",
    "\n",
    "# basic type split\n",
    "num_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(f\"Feature split -> numeric: {len(num_cols)}, categorical: {len(cat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f14de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "def build_pipeline(classifier):\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", SkPipeline([\n",
    "                (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"sc\", StandardScaler())\n",
    "            ]), num_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False), cat_cols),\n",
    "        ]\n",
    "    )\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(K_SELECTED, max(1, len(num_cols) + len(cat_cols))))\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"pre\", pre),\n",
    "        (\"adasyn\", ADASYN(random_state=RANDOM_STATE)),\n",
    "        (\"sel\", selector),\n",
    "        (\"clf\", classifier),\n",
    "    ])\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Define models ===\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400, max_depth=None, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "svm = SVC(\n",
    "    kernel=\"rbf\", C=2.0, gamma=\"scale\", probability=True, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "if HAS_XGB:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=500, max_depth=5, subsample=0.9, colsample_bytree=0.9,\n",
    "        learning_rate=0.05, objective=\"multi:softprob\", num_class=3,\n",
    "        random_state=RANDOM_STATE, eval_metric=\"mlogloss\", tree_method=\"hist\"\n",
    "    )\n",
    "else:\n",
    "    xgb = None\n",
    "\n",
    "estimators_for_stack = []\n",
    "estimators_for_stack.append((\"rf\", RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)))\n",
    "if HAS_XGB:\n",
    "    estimators_for_stack.append((\"xgb\", XGBClassifier(\n",
    "        n_estimators=300, max_depth=4, learning_rate=0.05, subsample=0.9,\n",
    "        colsample_bytree=0.9, objective=\"multi:softprob\", num_class=3,\n",
    "        random_state=RANDOM_STATE, eval_metric=\"mlogloss\", tree_method=\"hist\"\n",
    "    )))\n",
    "estimators_for_stack.append((\"svm\", SVC(kernel=\"rbf\", C=2.0, gamma=\"scale\", probability=True, random_state=RANDOM_STATE)))\n",
    "\n",
    "stack_final = LogisticRegression(max_iter=200, multi_class=\"ovr\")\n",
    "stack = StackingClassifier(estimators=estimators_for_stack, final_estimator=stack_final, cv=3, stack_method=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(name, base_clf):\n",
    "    pipe = build_pipeline(base_clf)\n",
    "    cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # probability predictions for ROC-AUC\n",
    "    y_proba = cross_val_predict(pipe, X, y, cv=cv, method=\"predict_proba\", n_jobs=-1)\n",
    "    y_pred  = y_proba.argmax(axis=1)\n",
    "\n",
    "    acc  = accuracy_score(y, y_pred)\n",
    "    f1   = f1_score(y, y_pred, average=\"macro\")\n",
    "\n",
    "    # macro ROC-AUC (one-vs-rest)\n",
    "    y_bin = label_binarize(y, classes=[0,1,2])\n",
    "    try:\n",
    "        auc = roc_auc_score(y_bin, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "\n",
    "    # confusion matrix (on CV preds)\n",
    "    cm = confusion_matrix(y, y_pred, labels=[0,1,2])\n",
    "\n",
    "    # plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"CN\",\"MCI\",\"DEMENTIA\"])\n",
    "    disp.plot(ax=ax[0], colorbar=False)\n",
    "    ax[0].set_title(f\"{name} — Confusion Matrix (CV)\")\n",
    "\n",
    "    ax[1].bar([\"Accuracy\",\"Macro-F1\",\"ROC-AUC\"], [acc, f1, auc])\n",
    "    ax[1].set_ylim(0,1)\n",
    "    ax[1].set_title(f\"{name} — Metrics (CV)\")\n",
    "    for i, v in enumerate([acc, f1, auc]):\n",
    "        ax[1].text(i, v+0.02 if v<=0.95 else v-0.08, f\"{v:.3f}\", ha=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # save\n",
    "    out_txt = OUTDIR / f\"{name.replace(' ','_').lower()}_metrics.txt\"\n",
    "    with open(out_txt, \"w\") as f:\n",
    "        f.write(f\"{name} (CV={N_FOLDS})\\n\")\n",
    "        f.write(f\"Accuracy: {acc:.4f}\\nMacro-F1: {f1:.4f}\\nMacro ROC-AUC: {auc:.4f}\\n\")\n",
    "        f.write(\"\\nConfusion matrix (rows=true, cols=pred):\\n\")\n",
    "        f.write(str(cm))\n",
    "    print(\"Saved metrics ->\", out_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a73c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\n=== Random Forest ===\")\n",
    "evaluate_model(\"Random Forest\", rf)\n",
    "\n",
    "if HAS_XGB:\n",
    "    print(\"\\n=== XGBoost ===\")\n",
    "    evaluate_model(\"XGBoost\", xgb)\n",
    "else:\n",
    "    print(\"\\n[Skip] XGBoost not available in this environment\")\n",
    "\n",
    "print(\"\\n=== SVM (RBF) ===\")\n",
    "evaluate_model(\"SVM (RBF)\", svm)\n",
    "\n",
    "print(\"\\n=== Stacking Ensemble ===\")\n",
    "evaluate_model(\"Stacking Ensemble\", stack)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
