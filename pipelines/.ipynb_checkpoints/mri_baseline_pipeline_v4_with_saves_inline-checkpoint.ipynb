{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455dbcba",
   "metadata": {},
   "source": [
    "\n",
    "# MRI Baseline Pipeline (ADNI) — v3 (unsupervised filtering)\n",
    "\n",
    "**What changed from v2**\n",
    "- **No dependency** on the must-keep Excel workbook — a **token-based must-keep list** is embedded.\n",
    "- **Do not restrict** to 58 columns. Instead, we do **modality-level unsupervised filtering** across all MRI features and then **additionally keep** the must-keep set + computed ratios.\n",
    "- **Ratios** are computed via **token-based column discovery** (Hipp/ICV, Vent/ICV, lobar/ICV, Hipp/Parahipp, AIs).\n",
    "\n",
    "**Inputs expected** (in the working directory):\n",
    "- `MRI_data ADNI.xlsx`\n",
    "\n",
    "**Outputs**\n",
    "- `data/processed/mri_baseline_core.xlsx` — only IDs + must-keep features (post-QC, pre-imputation) for reference\n",
    "- `data/processed/mri_imputed_core.xlsx` — full final table: IDs + kept features (must-keep + filtered additional) + computed ratios (post-imputation)\n",
    "- Plots under `data/processed/plots/mri/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, math, json, warnings, itertools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- I/O config ---\n",
    "#MRI_FILE = \"MRI_data ADNI.xlsx\"\n",
    "PROJECT_ROOT = Path(\"/Users/madhurabn/Desktop/adni\")\n",
    "MRI_FILE  = PROJECT_ROOT / \"data\" / \"raw\" / \"MRI_data ADNI.xlsx\"\n",
    "OUTDIR      = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "PLOTDIR  = OUTDIR / \"plots\" / \"mri\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert Path(MRI_FILE).exists(), f\"Missing MRI file: {MRI_FILE}\"\n",
    "print(\"Working directory:\", Path('.').resolve())\n",
    "print(\"MRI file size:\", Path(MRI_FILE).stat().st_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc9df3",
   "metadata": {},
   "source": [
    "## Step 1 — Load MRI & earliest-visit selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2362644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mri = pd.read_excel(MRI_FILE)\n",
    "print(\"MRI shape:\", mri.shape)\n",
    "if \"PTID\" not in mri.columns or \"visit\" not in mri.columns:\n",
    "    raise KeyError(\"MRI must contain 'PTID' and 'visit'. Found: \" + \", \".join(mri.columns[:20]))\n",
    "\n",
    "visit_priority = [\"bl\",\"init\",\"sc\",\"m03\",\"m06\",\"m12\",\"m24\",\"m36\",\"m48\",\"m60\",\"m72\"]\n",
    "def norm_visit(v):\n",
    "    if pd.isna(v): return \"zzz\"\n",
    "    s = str(v).strip().lower()\n",
    "    s = s.replace(\"initial\",\"init\")\n",
    "    return s\n",
    "order_map = {v:i for i,v in enumerate(visit_priority)}\n",
    "mri[\"visit_norm\"]  = mri[\"visit\"].map(norm_visit)\n",
    "mri[\"visit_order\"] = mri[\"visit_norm\"].map(lambda v: order_map.get(v, 999))\n",
    "mri_bl = (mri\n",
    "          .sort_values([\"PTID\",\"visit_order\"])\n",
    "          .drop_duplicates(\"PTID\", keep=\"first\")\n",
    "          .drop(columns=[\"visit_norm\",\"visit_order\"]))\n",
    "print(\"After earliest-visit filter:\", mri_bl.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fcdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save baseline (after earliest-visit filter) and print quick stats ---\n",
    "from pathlib import Path\n",
    "\n",
    "OUTDIR   = Path(\"data/processed\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "baseline_after_visit = OUTDIR / \"mri_after_visit.xlsx\"\n",
    "mri_bl.to_excel(baseline_after_visit, index=False)\n",
    "print(\"Saved baseline (after earliest visit):\", baseline_after_visit)\n",
    "\n",
    "# --- Quick statistics for baseline table ---\n",
    "print(\"\\n=== Baseline cohort statistics ===\")\n",
    "print(\"Number of patients:\", mri_bl[\"PTID\"].nunique())\n",
    "print(\"Visits included:\", mri_bl[\"visit\"].value_counts().to_dict())\n",
    "print(\"Rows:\", mri_bl.shape[0], \" | Columns:\", mri_bl.shape[1])\n",
    "\n",
    "miss_pct_bl = mri_bl.isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features by missingness:\")\n",
    "print(miss_pct_bl.head(10))\n",
    "\n",
    "import numpy as np\n",
    "num_cols_bl = [c for c in mri_bl.columns if np.issubdtype(mri_bl[c].dtype, np.number)]\n",
    "if num_cols_bl:\n",
    "    try:\n",
    "        display(mri_bl[num_cols_bl].describe().T.head(5))\n",
    "    except Exception:\n",
    "        print(mri_bl[num_cols_bl].describe().T.head(5))\n",
    "else:\n",
    "    print(\"No numeric columns found at this stage.\")\n",
    "\n",
    "must_keep_overlap = [c for c in mri_bl.columns if any(tok in c for tok in [\"Hippocampus\",\"Amygdala\",\"Thalamus\",\"Caudate\",\"Putamen\",\"Ventricle\",\"ICV\",\"Icv\"])]\n",
    "print(\"\\nFound must-keep overlaps in baseline:\", len(must_keep_overlap))\n",
    "print(must_keep_overlap[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick statistics for baseline table ---\n",
    "print(\"\\n=== Baseline cohort statistics ===\")\n",
    "print(\"Number of patients:\", mri_bl[\"PTID\"].nunique())\n",
    "print(\"Visits included:\", mri_bl[\"visit\"].value_counts().to_dict())\n",
    "\n",
    "# Overall shape\n",
    "print(\"Rows:\", mri_bl.shape[0], \" | Columns:\", mri_bl.shape[1])\n",
    "\n",
    "# Missingness summary (top 10)\n",
    "miss_pct = mri_bl.isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features by missingness:\")\n",
    "print(miss_pct.head(10))\n",
    "\n",
    "# Numeric summary (basic stats for first few numeric columns)\n",
    "num_cols = [c for c in mri_bl.columns if np.issubdtype(mri_bl[c].dtype, np.number)]\n",
    "if num_cols:\n",
    "    print(\"\\nDescriptive stats (first 5 numeric columns):\")\n",
    "    display(mri_bl[num_cols].describe().T.head(5))\n",
    "else:\n",
    "    print(\"\\nNo numeric columns found at this stage.\")\n",
    "\n",
    "# Check overlap of must-keep tokens (sanity check)\n",
    "must_keep_overlap = [c for c in mri_bl.columns if any(tok in c for tok in [\"Hippocampus\",\"Amygdala\",\"Thalamus\",\"Caudate\",\"Putamen\",\"Ventricle\",\"ICV\"])]\n",
    "print(\"\\nFound must-keep overlaps in baseline:\", len(must_keep_overlap))\n",
    "print(must_keep_overlap[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542ef1a",
   "metadata": {},
   "source": [
    "## Step 2 — QC handling (overall + partial ROI blanking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_fail(x):\n",
    "    s = str(x).strip().lower()\n",
    "    return s in {\"fail\",\"0\",\"false\",\"f\"}\n",
    "\n",
    "mri_qc = mri_bl.copy()\n",
    "\n",
    "# OVERALLQC drop\n",
    "if \"OVERALLQC\" in mri_qc.columns:\n",
    "    before = mri_qc.shape[0]\n",
    "    mri_qc = mri_qc[~mri_qc[\"OVERALLQC\"].map(is_fail)].copy()\n",
    "    print(f\"Dropped {before - mri_qc.shape[0]} rows due to OVERALLQC=Fail\")\n",
    "else:\n",
    "    print(\"OVERALLQC not found; skipping row drop.\")\n",
    "\n",
    "# Partial QC families (token-based match on verbose ADNI headers)\n",
    "qc_family = {\n",
    "    \"TEMPQC\":  [\"TemporalPole\",\"Fusiform\",\"SuperiorTemporal\",\"MiddleTemporal\",\"InferiorTemporal\"],\n",
    "    \"FRONTQC\": [\"FrontalPole\",\"Precentral\",\"SuperiorFrontal\",\"CaudalMiddleFrontal\",\"RostralMiddleFrontal\",\"MedialOrbitofrontal\"],\n",
    "    \"PARQC\":   [\"SuperiorParietal\",\"InferiorParietal\",\"Supramarginal\",\"Precuneus\",\"Postcentral\",\"Paracentral\"],\n",
    "    \"INSULAQC\":[\"Insula\"],\n",
    "    \"OCCQC\":   [\"LateralOccipital\",\"Cuneus\",\"Lingual\",\"Pericalcarine\"],\n",
    "    \"BGQC\":    [\"Caudate\",\"Putamen\",\"Pallidum\",\"Thalamus\",\"Accumbens\",\"Amygdala\",\"Hippocampus\"],\n",
    "    \"CWMQC\":   [\"WhiteMatter\"],\n",
    "    \"VENTQC\":  [\"LateralVentricle\",\"InferiorLateralVentricle\",\"ThirdVentricle\"],\n",
    "    \"HIPPOQC\": [\"Hippocampus\"]\n",
    "}\n",
    "\n",
    "for qc, tokens in qc_family.items():\n",
    "    if qc in mri_qc.columns:\n",
    "        mask = mri_qc[qc].map(is_fail)\n",
    "        if mask.any():\n",
    "            affected_cols = [c for c in mri_qc.columns if any(t in c for t in tokens)]\n",
    "            if affected_cols:\n",
    "                mri_qc.loc[mask, affected_cols] = np.nan\n",
    "                print(f\"{qc}: set NaN in {len(affected_cols)} ROI cols for {mask.sum()} rows\")\n",
    "            else:\n",
    "                print(f\"{qc}: no matching ROI columns found for tokens={tokens}\")\n",
    "print(\"QC handling complete. Shape:\", mri_qc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9d516",
   "metadata": {},
   "source": [
    "## Step 3 — Embedded must-keep set (token-matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We define concept-level must-keep tokens that match ADNI's verbose headers.\n",
    "# The scanner will keep every column whose name contains ANY of these token patterns.\n",
    "# This approximates the 58 'core' without needing an external workbook.\n",
    "\n",
    "MUST_KEEP_TOKEN_GROUPS = [\n",
    "    # Subcortical volumes (L/R)\n",
    "    [\"Hippocampus\"], [\"Amygdala\"], [\"Thalamus\"], [\"Caudate\"], [\"Putamen\"],\n",
    "    [\"LateralVentricle\"], [\"InferiorLateralVentricle\"],\n",
    "    # AD-signature cortical means (aparc thickness/volume if present)\n",
    "    [\"Entorhinal\"], [\"Parahippocampal\"], [\"Precuneus\"], [\"PosteriorCingulate\"],\n",
    "    # Global measures\n",
    "    [\"EstimatedTotalIntraCranialVol\",\"eTIV\",\"ICV\"], [\"MeanThickness\"], [\"TotalCorticalGrayMatter\",\"Cortical Gray Matter\"],\n",
    "    # White matter hypointensities / if present\n",
    "    [\"WhiteMatterHypointensities\",\"WM-hypointensities\",\"White Matter Hypointensities\"],\n",
    "]\n",
    "\n",
    "all_cols = list(mri_qc.columns)\n",
    "def match_any_token_group(colname: str, groups) -> bool:\n",
    "    return any(all(tok in colname for tok in group) for group in groups)\n",
    "\n",
    "must_keep_cols = [\"PTID\",\"visit\"]\n",
    "for c in all_cols:\n",
    "    if match_any_token_group(c, MUST_KEEP_TOKEN_GROUPS):\n",
    "        must_keep_cols.append(c)\n",
    "must_keep_cols = list(dict.fromkeys(must_keep_cols))  # de-dup, preserve order\n",
    "\n",
    "print(f\"Matched must-keep columns: {len(must_keep_cols)}\")\n",
    "print(must_keep_cols[:10], '...')\n",
    "mri_core = mri_qc[must_keep_cols].copy()\n",
    "print(\"mri_core shape (IDs + must-keep):\", mri_core.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75706f",
   "metadata": {},
   "source": [
    "## Step 4 — EDA (pre-imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Missingness bar (top 30 among must-keep)\n",
    "miss_pct = mri_core.isna().mean().sort_values(ascending=False)\n",
    "plt.figure()\n",
    "miss_pct.head(30).plot(kind=\"bar\")\n",
    "plt.title(\"Top-30 missingness (pre, must-keep)\")\n",
    "plt.tight_layout(); plt.savefig(PLOTDIR / \"pre_missingness_mustkeep_top30.png\", dpi=150); plt.show()\n",
    "\n",
    "# OVERALLQC distribution on raw\n",
    "if \"OVERALLQC\" in mri.columns:\n",
    "    plt.figure()\n",
    "    mri[\"OVERALLQC\"].astype(str).value_counts(dropna=False).plot(kind=\"bar\")\n",
    "    plt.title(\"OVERALLQC distribution (raw)\")\n",
    "    plt.tight_layout(); plt.savefig(PLOTDIR / \"qc_overall_distribution.png\", dpi=150); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa95473",
   "metadata": {},
   "source": [
    "## Step 5 — Modality-level unsupervised filtering on **all MRI features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dbe5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We filter across the full MRI modality (not just must-keep).\n",
    "# Rules (unsupervised — no labels):\n",
    "# 1) Drop columns with > MAX_MISS missingness (after QC blanking).\n",
    "# 2) Drop constant / near-constant columns (numeric: std == 0; categorical: dominant level > NZV_THRESH).\n",
    "# 3) Drop exact duplicate columns.\n",
    "# 4) Redundancy pruning: Spearman |r| >= HIGH_R -> drop one from each pair (protect must-keep).\n",
    "\n",
    "MAX_MISS   = 0.60\n",
    "NZV_THRESH = 0.98    # near-zero variance for categoricals\n",
    "HIGH_R     = 0.98    # redundancy threshold for numerics\n",
    "\n",
    "df_all = mri_qc.copy()\n",
    "\n",
    "# 1) Missingness filter\n",
    "miss = df_all.isna().mean()\n",
    "pass_miss = miss.index[miss <= MAX_MISS].tolist()\n",
    "dropped_miss = [c for c in df_all.columns if c not in pass_miss]\n",
    "print(f\"Missingness filter: kept {len(pass_miss)}, dropped {len(dropped_miss)} (> {MAX_MISS*100:.0f}% missing)\")\n",
    "\n",
    "df1 = df_all[pass_miss].copy()\n",
    "\n",
    "# 2) Near-constant\n",
    "num_cols = [c for c in df1.columns if c not in [\"PTID\",\"visit\"] and np.issubdtype(df1[c].dtype, np.number)]\n",
    "cat_cols = [c for c in df1.columns if c not in [\"PTID\",\"visit\"] and c not in num_cols]\n",
    "\n",
    "keep_nc = []\n",
    "drop_nc = []\n",
    "for c in df1.columns:\n",
    "    if c in [\"PTID\",\"visit\"]:\n",
    "        keep_nc.append(c); continue\n",
    "    s = df1[c]\n",
    "    if c in num_cols:\n",
    "        if s.nunique(dropna=True) <= 1 or (float(np.nanstd(s.values)) == 0.0):\n",
    "            drop_nc.append(c)\n",
    "        else:\n",
    "            keep_nc.append(c)\n",
    "    else:\n",
    "        vc = s.astype(str).value_counts(dropna=False)\n",
    "        if not vc.empty and (vc.iloc[0] / max(1, vc.sum())) >= NZV_THRESH:\n",
    "            drop_nc.append(c)\n",
    "        else:\n",
    "            keep_nc.append(c)\n",
    "print(f\"Near-constant: kept {len(keep_nc)}, dropped {len(drop_nc)}\")\n",
    "df2 = df1[keep_nc].copy()\n",
    "\n",
    "# 3) Exact duplicate columns\n",
    "def deduplicate_columns(df):\n",
    "    seen = {}\n",
    "    keep = []\n",
    "    dupes = []\n",
    "    for c in df.columns:\n",
    "        key = tuple(pd.util.hash_pandas_object(df[c], index=False).values)\n",
    "        if key in seen:\n",
    "            dupes.append(c)\n",
    "        else:\n",
    "            seen[key] = c\n",
    "            keep.append(c)\n",
    "    return df[keep].copy(), dupes\n",
    "\n",
    "df3, dup_cols = deduplicate_columns(df2)\n",
    "print(f\"Duplicate columns dropped: {len(dup_cols)}\")\n",
    "\n",
    "# 4) Redundancy pruning on numerics (Spearman), with must-keep protection\n",
    "num_cols3 = [c for c in df3.columns if c not in [\"PTID\",\"visit\"] and np.issubdtype(df3[c].dtype, np.number)]\n",
    "corr = df3[num_cols3].corr(method=\"spearman\")\n",
    "to_drop = set()\n",
    "protected = set(must_keep_cols)\n",
    "\n",
    "for i, c1 in enumerate(num_cols3):\n",
    "    if c1 in to_drop: \n",
    "        continue\n",
    "    for c2 in num_cols3[i+1:]:\n",
    "        if c2 in to_drop:\n",
    "            continue\n",
    "        r = corr.loc[c1, c2]\n",
    "        if pd.notna(r) and abs(r) >= HIGH_R:\n",
    "            # Drop the one that is NOT protected; if both protected, keep both.\n",
    "            if c1 in protected and c2 not in protected:\n",
    "                to_drop.add(c2)\n",
    "            elif c2 in protected and c1 not in protected:\n",
    "                to_drop.add(c1)\n",
    "            elif (c1 not in protected) and (c2 not in protected):\n",
    "                # Drop the one with higher missingness historically (from df_all)\n",
    "                miss1 = miss.get(c1, 0.0); miss2 = miss.get(c2, 0.0)\n",
    "                drop_c = c1 if miss1 >= miss2 else c2\n",
    "                to_drop.add(drop_c)\n",
    "\n",
    "print(f\"Redundancy pruning: to_drop={len(to_drop)} at |r|>={HIGH_R}\")\n",
    "filtered_cols = [c for c in df3.columns if c not in to_drop]\n",
    "print(f\"Filtered feature set size (pre-imputation): {len(filtered_cols)}\")\n",
    "df_filtered = df3[filtered_cols].copy()\n",
    "\n",
    "# Always ensure IDs present\n",
    "if \"PTID\" not in df_filtered.columns: df_filtered.insert(0, \"PTID\", mri_qc[\"PTID\"].values)\n",
    "if \"visit\" not in df_filtered.columns: df_filtered.insert(1, \"visit\", mri_qc[\"visit\"].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5ff6f",
   "metadata": {},
   "source": [
    "## Step 6 — Imputation (mode for categoricals, MICE for numerics) + IQR clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ccef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_cols = [\"PTID\",\"visit\"]\n",
    "feat_cols = [c for c in df_filtered.columns if c not in id_cols]\n",
    "\n",
    "num_cols_f = [c for c in feat_cols if np.issubdtype(df_filtered[c].dtype, np.number)]\n",
    "cat_cols_f = [c for c in feat_cols if c not in num_cols_f]\n",
    "\n",
    "df_preimp = df_filtered.copy()\n",
    "\n",
    "# Mode impute categoricals\n",
    "if cat_cols_f:\n",
    "    mode_imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "    df_preimp[cat_cols_f] = mode_imp.fit_transform(df_preimp[cat_cols_f])\n",
    "\n",
    "# MICE for numerics\n",
    "df_imp = df_preimp.copy()\n",
    "if num_cols_f:\n",
    "    mice = IterativeImputer(random_state=42, max_iter=10, initial_strategy=\"median\")\n",
    "    df_imp[num_cols_f] = mice.fit_transform(df_imp[num_cols_f])\n",
    "\n",
    "# IQR clipping + non-negative guard\n",
    "def iqr_bounds(a, k=1.5):\n",
    "    q1 = np.nanpercentile(a, 25); q3 = np.nanpercentile(a, 75); iqr = q3 - q1\n",
    "    return q1 - k*iqr, q3 + k*iqr\n",
    "\n",
    "volume_like_tokens = [\"volume\",\"surface area\",\"thickness\",\"ventricle\",\"hippocampus\",\"amygdala\",\"putamen\",\"caudate\",\"thalamus\",\"pallidum\",\"icv\"]\n",
    "for c in num_cols_f:\n",
    "    arr = df_imp[c].values\n",
    "    lo, hi = iqr_bounds(arr)\n",
    "    arr = np.clip(arr, lo, hi)\n",
    "    if any(tok in c.lower() for tok in volume_like_tokens):\n",
    "        arr = np.maximum(arr, 0)\n",
    "    df_imp[c] = arr\n",
    "\n",
    "print(\"Imputation complete. Shape:\", df_imp.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4105a57",
   "metadata": {},
   "source": [
    "## Step 7 — Ratios (token-based discovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88971ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_first(cols, tokens):\n",
    "    # Return first column whose name contains ALL tokens\n",
    "    for c in cols:\n",
    "        name = c.lower()\n",
    "        if all(tok.lower() in name for tok in tokens):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "cols = list(df_imp.columns)\n",
    "\n",
    "# Likely tokens for common fields\n",
    "ICV_COL = (find_first(cols, [\"EstimatedTotalIntraCranialVol\"]) or\n",
    "           find_first(cols, [\"eTIV\"]) or\n",
    "           find_first(cols, [\"icv\"]) or\n",
    "           find_first(cols, [\"total intracranial\"]))\n",
    "\n",
    "# Hippocampus L/R (volume)\n",
    "HIPPO_L = find_first(cols, [\"Hippocampus\",\"Left\"])\n",
    "HIPPO_R = find_first(cols, [\"Hippocampus\",\"Right\"])\n",
    "\n",
    "# Lateral Ventricles L/R\n",
    "VENT_L  = find_first(cols, [\"LateralVentricle\",\"Left\"])\n",
    "VENT_R  = find_first(cols, [\"LateralVentricle\",\"Right\"])\n",
    "\n",
    "# Parahippocampal L/R (volume or thickness)\n",
    "PARAH_L = find_first(cols, [\"Parahippocamp\",\"Left\"])\n",
    "PARAH_R = find_first(cols, [\"Parahippocamp\",\"Right\"])\n",
    "\n",
    "# Lobar volumes (examples)\n",
    "FRONTAL_T = find_first(cols, [\"Frontal\",\"Cortical Volume\"]) or find_first(cols, [\"Frontal\",\"Total Volume\"])\n",
    "TEMPORAL_T= find_first(cols, [\"Temporal\",\"Cortical Volume\"]) or find_first(cols, [\"Temporal\",\"Total Volume\"])\n",
    "PARIETAL_T= find_first(cols, [\"Parietal\",\"Cortical Volume\"]) or find_first(cols, [\"Parietal\",\"Total Volume\"])\n",
    "OCCIP_T   = find_first(cols, [\"Occipital\",\"Cortical Volume\"]) or find_first(cols, [\"Occipital\",\"Total Volume\"])\n",
    "\n",
    "def safe_ratio(a, b):\n",
    "    a = a.astype(float); b = b.astype(float)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        r = a / b\n",
    "    r[~np.isfinite(r)] = np.nan\n",
    "    return r\n",
    "\n",
    "added = []\n",
    "\n",
    "# Hipp/ICV\n",
    "if HIPPO_L and HIPPO_R and ICV_COL:\n",
    "    df_imp[\"HippICV_L\"] = safe_ratio(df_imp[HIPPO_L], df_imp[ICV_COL]); added.append(\"HippICV_L\")\n",
    "    df_imp[\"HippICV_R\"] = safe_ratio(df_imp[HIPPO_R], df_imp[ICV_COL]); added.append(\"HippICV_R\")\n",
    "    df_imp[\"HippICV_Total\"] = safe_ratio(df_imp[HIPPO_L] + df_imp[HIPPO_R], df_imp[ICV_COL]); added.append(\"HippICV_Total\")\n",
    "    denom = (df_imp[HIPPO_L] + df_imp[HIPPO_R]).replace(0, np.nan)\n",
    "    df_imp[\"Hipp_AI\"] = safe_ratio(df_imp[HIPPO_R] - df_imp[HIPPO_L], denom); added.append(\"Hipp_AI\")\n",
    "\n",
    "# Vent/ICV\n",
    "if VENT_L and VENT_R and ICV_COL:\n",
    "    df_imp[\"VentICV_L\"] = safe_ratio(df_imp[VENT_L], df_imp[ICV_COL]); added.append(\"VentICV_L\")\n",
    "    df_imp[\"VentICV_R\"] = safe_ratio(df_imp[VENT_R], df_imp[ICV_COL]); added.append(\"VentICV_R\")\n",
    "    df_imp[\"VentICV_Total\"] = safe_ratio(df_imp[VENT_L] + df_imp[VENT_R], df_imp[ICV_COL]); added.append(\"VentICV_Total\")\n",
    "\n",
    "# Lobar/ICV (if present)\n",
    "for nm, col in [(\"FrontalICV_Total\", FRONTAL_T), (\"TemporalICV_Total\", TEMPORAL_T),\n",
    "                (\"ParietalICV_Total\", PARIETAL_T), (\"OccipitalICV_Total\", OCCIP_T)]:\n",
    "    if col and ICV_COL:\n",
    "        df_imp[nm] = safe_ratio(df_imp[col], df_imp[ICV_COL]); added.append(nm)\n",
    "\n",
    "# Hipp/Parahipp (Total)\n",
    "if HIPPO_L and HIPPO_R and PARAH_L and PARAH_R:\n",
    "    num = (df_imp[HIPPO_L] + df_imp[HIPPO_R]).astype(float)\n",
    "    den = (df_imp[PARAH_L] + df_imp[PARAH_R]).astype(float)\n",
    "    df_imp[\"Hipp_over_Parahipp_Total\"] = safe_ratio(num, den); added.append(\"Hipp_over_Parahipp_Total\")\n",
    "\n",
    "print(\"Added ratio columns:\", added)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e7527",
   "metadata": {},
   "source": [
    "## Step 8 — EDA (post-imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "miss_post = df_imp.isna().mean().sort_values(ascending=False)\n",
    "plt.figure(); miss_post.head(30).plot(kind=\"bar\")\n",
    "plt.title(\"Top-30 missingness (post-imputation)\")\n",
    "plt.tight_layout(); plt.savefig(PLOTDIR / \"post_missingness_top30.png\", dpi=150); plt.show()\n",
    "\n",
    "# Simple correlation on numeric block (post)\n",
    "num_cols_post = [c for c in df_imp.columns if c not in [\"PTID\",\"visit\"] and np.issubdtype(df_imp[c].dtype, np.number)]\n",
    "if len(num_cols_post) >= 3:\n",
    "    corr_post = df_imp[num_cols_post].corr(method=\"spearman\")\n",
    "    plt.figure()\n",
    "    plt.imshow(corr_post.values, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    plt.title(\"Spearman correlation (post)\")\n",
    "    plt.colorbar(); plt.tight_layout(); plt.savefig(PLOTDIR / \"post_corr_all.png\", dpi=150); plt.show()\n",
    "\n",
    "# Example scatter if ratios present\n",
    "if \"HippICV_Total\" in df_imp.columns and \"VentICV_Total\" in df_imp.columns:\n",
    "    x = df_imp[\"HippICV_Total\"].values; y = df_imp[\"VentICV_Total\"].values\n",
    "    plt.figure(); plt.scatter(x, y, s=8)\n",
    "    plt.xlabel(\"HippICV_Total\"); plt.ylabel(\"VentICV_Total\")\n",
    "    plt.title(\"Hipp/ICV vs Vent/ICV\")\n",
    "    plt.tight_layout(); plt.savefig(PLOTDIR / \"scatter_hippICV_vs_ventICV.png\", dpi=150); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0c7f8",
   "metadata": {},
   "source": [
    "## Step 9 — Final selection = must-keep + computed ratios + filtered additional ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build final column list:\n",
    "computed_cols = [c for c in df_imp.columns if c.endswith(\"_AI\") or \"ICV_\" in c or c.endswith(\"ICV_Total\") or c.startswith(\"Hipp_over_\")]\n",
    "final_keep = list(dict.fromkeys([\"PTID\",\"visit\"] + must_keep_cols + computed_cols))\n",
    "\n",
    "# Add filtered additional numeric ROIs that survived filtering & imputation\n",
    "for c in df_imp.columns:\n",
    "    if c in final_keep: \n",
    "        continue\n",
    "    if c in [\"PTID\",\"visit\"]:\n",
    "        continue\n",
    "    # Keep additional ROI-like numeric features (heuristic: contains ROI-like tokens)\n",
    "    if np.issubdtype(df_imp[c].dtype, np.number):\n",
    "        if any(tok in c for tok in [\"Volume\",\"Thickness\",\"Surface Area\",\"Ventricle\",\"Hippocampus\",\"Amygdala\",\"Thalamus\",\"Putamen\",\"Caudate\",\"Pallidum\"]):\n",
    "            final_keep.append(c)\n",
    "\n",
    "df_final = df_imp[final_keep].copy()\n",
    "print(\"Final table shape:\", df_final.shape)\n",
    "print(\"Final columns (head):\", df_final.columns[:15].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ff4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save final (post-imputation: must-keep + ratios + filtered additional ROIs) ---\n",
    "imputed_out = Path(\"data/processed\") / \"mri_imputed_core.xlsx\"\n",
    "df_final.to_excel(imputed_out, index=False)\n",
    "print(\"Saved final imputed table:\", imputed_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e6918",
   "metadata": {},
   "source": [
    "## Step 10 — Save tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d389dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "core_out    = OUTDIR / \"mri_baseline_core.xlsx\"\n",
    "imputed_out = OUTDIR / \"mri_imputed_core.xlsx\"\n",
    "\n",
    "# Core = IDs + must-keep (pre-imputation snapshot already saved earlier? Here we save from mri_core)\n",
    "mri_core.to_excel(core_out, index=False)\n",
    "\n",
    "# Final = IDs + must-keep + computed ratios + filtered additional (post-imputation)\n",
    "df_final.to_excel(imputed_out, index=False)\n",
    "\n",
    "print(\"Saved:\", core_out, \"and\", imputed_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save baseline core (post-QC, IDs + must-keep) ---\n",
    "core_out = Path(\"data/processed\") / \"mri_baseline_core.xlsx\"\n",
    "mri_core.to_excel(core_out, index=False)\n",
    "print(\"Saved baseline core:\", core_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
